{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QUjBEMGxjGJ"
      },
      "source": [
        "# 0. Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWVuw6RWxi8d"
      },
      "outputs": [],
      "source": [
        "model_distributer = \"meta-llama\"\n",
        "model_name = \"Llama-3.1-8B\"\n",
        "root_path = \"./data\"\n",
        "save_path = root_path +\"/\"+ model_name.lower()\n",
        "dataset = [\"CommonsenseQA_test.jsonl\",]\n",
        "dataset_path = root_path + \"/\" + dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FAtcUCJEyTxs",
        "outputId": "d0c25509-648f-4d18-e0aa-551d2e3f5bfd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/KAIRI_Experiment/Prompt_Bias/llama-3.1-8b'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z6NBsUKbHEn"
      },
      "source": [
        "# 1. üóû Data and Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqfhxxvShZBI"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBwpXtfIZAzb"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8lFRHKbdzTN",
        "outputId": "4ec762a7-6e32-48fe-8be1-fe77d11e5179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.3.0)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YniXgnwLbp9f"
      },
      "outputs": [],
      "source": [
        "import jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hWHu15BdCmR"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with jsonlines.open(dataset_path) as json_file:\n",
        "  for line in json_file:\n",
        "    data.append(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F4RJLhHhb3_"
      },
      "source": [
        "## Make prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q5Aaxr0dceT"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "\"\"\"question:{question}\n",
        "options:{options}\n",
        "answer:\"\"\",\n",
        "\"\"\"Question:{question}\n",
        "Options:{options}\n",
        "Answer:\"\"\",\n",
        "\"\"\"QUESTION:{question}\n",
        "OPTIONS:{options}\n",
        "ANSWER:\"\"\",\n",
        "\"\"\" question: {question}\n",
        " options: {options}\n",
        " answer:\"\"\",\n",
        "\"\"\" Question: {question}\n",
        " Options: {options}\n",
        " Answer:\"\"\",\n",
        "\"\"\" QUESTION: {question}\n",
        " OPTIONS: {options}\n",
        " ANSWER:\"\"\",]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_WCBnifhfEY"
      },
      "source": [
        "# 2. üëΩ Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja5Mjo7dgWG6"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vITYIF6DoxKi"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if \"Llama\" in model_name:\n",
        "  model = LlamaForCausalLM.from_pretrained(model_distributer+\"/\"+model_name, torch_dtype=torch.float16)\n",
        "elif \"gemma\" in model_name:\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_distributer+\"/\"+model_name, torch_dtype=torch.float16)\n",
        "else:\n",
        "  try:\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_distributer+\"/\"+model_name, torch_dtype=torch.float16)\n",
        "  except:\n",
        "    raise ValueError(\"The model is not supported.\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_distributer+\"/\"+model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQXmdTh6sL5Y"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTFQ-YSnXmqp"
      },
      "outputs": [],
      "source": [
        "model_unembed = model.lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uGF8Z7X0KTl"
      },
      "source": [
        "# 3. Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQRMn5cY0P5X"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Union, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onJIKUuUt5sy"
      },
      "outputs": [],
      "source": [
        "def apply_content(template: str, content: dict) -> str:\n",
        "  \"\"\"\n",
        "  Fills a specified prompt template with content from a single data entry\n",
        "  (e.g., from a JSON Lines file).\n",
        "\n",
        "  Args:\n",
        "      template (str): A string template selected from multiple prompt candidates.\n",
        "                      This template must contain {question} and {options} placeholders.\n",
        "      content (dict): A JSON object read from a line in a .jsonl file,\n",
        "                      containing information like questions and choices.\n",
        "\n",
        "  Returns:\n",
        "      str: The completed prompt string with content applied.\n",
        "  \"\"\"\n",
        "  # Extract the question text from the content dictionary.\n",
        "  question = content[\"questions\"][\"original\"]\n",
        "\n",
        "  # Create the options list string using the 'options' list from content.\n",
        "  # Example: \"\\nA. Answer 1\\nB. Answer 2\"\n",
        "  options_str = \"\".join(\n",
        "      f\"\\n{option['label']}. {option['text']}\" for option in content[\"options\"]\n",
        "  )\n",
        "\n",
        "  # Replace the template's placeholders with the actual content and return the final string.\n",
        "  return template.format(question=question, options=options_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVaSRmnpMeUO"
      },
      "outputs": [],
      "source": [
        "for i in data:\n",
        "  if i['id'] == 'edd1634d911614590c6b8ca730df95fe':\n",
        "    for j in range(3):\n",
        "      print(apply_content(template=prompts[j], content=i))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQa7eIOL0Mue"
      },
      "outputs": [],
      "source": [
        "def make_tokenized_prompts(\n",
        "    templates: List[str],\n",
        "    json_data: Dict,\n",
        "    tokenizer: AutoTokenizer = tokenizer,\n",
        "    device: torch.device = device\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Applies json_data to a list of prompt templates and tokenizes each result.\n",
        "\n",
        "    Args:\n",
        "        templates (List[str]): A list of prompt templates to be filled.\n",
        "        json_data (Dict): The JSON object containing data to fill into the prompts.\n",
        "        tokenizer (AutoTokenizer): The tokenizer object to use.\n",
        "        device (torch.device): The device to move the tokenized tensors to.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: A list of tokenized prompt tensors (as dictionaries from the tokenizer).\n",
        "    \"\"\"\n",
        "    return [\n",
        "        tokenizer(apply_content(template, json_data), return_tensors = \"pt\").to(device)\n",
        "        for template in templates\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFH6C5aX02Ao"
      },
      "outputs": [],
      "source": [
        "def process_data(\n",
        "    model: Any, # PreTrainedModel\n",
        "    tokenized_prompts: List[Dict[str, Any]], # torch.Tensor\n",
        ") -> Dict[str, Dict[int, List[float]]]:\n",
        "  \"\"\"\n",
        "  Runs the model to extract the hidden states of the last token\n",
        "  for each prompt across all layers.\n",
        "\n",
        "  Args:\n",
        "      model (transformers.PreTrainedModel):\n",
        "          A pre-trained transformer model object from Hugging Face.\n",
        "          Must support the `output_hidden_states=True` option.\n",
        "\n",
        "      tokenized_prompts (List[Dict[str, torch.Tensor]]):\n",
        "          A list of tokenized prompt data.\n",
        "          Each element is a dictionary of the form `{'input_ids': torch.Tensor, ...}`.\n",
        "          - Example: `[{'input_ids': tensor([[101, 2054, ...]])}, {'input_ids': tensor([[101, 2500, ...]])}]`\n",
        "\n",
        "  Returns:\n",
        "      Dict[str, Dict[int, List[float]]]:\n",
        "          A dictionary containing the hidden states of the last token,\n",
        "          organized by prompt and by layer.\n",
        "          - Structure:\n",
        "            {\n",
        "                \"Prompt_1\": {\n",
        "                    0: [0.1, 0.2, ...],  # Hidden state vector for layer 0\n",
        "                    1: [0.3, 0.1, ...],  # Hidden state vector for layer 1\n",
        "                    ...\n",
        "                },\n",
        "                \"Prompt_2\": { ... }\n",
        "            }\n",
        "  \"\"\"\n",
        "  hidden_states_contents = {}\n",
        "  with torch.no_grad():\n",
        "    for i, item in enumerate(tokenized_prompts):\n",
        "      # Run the model\n",
        "      outputs = model(item['input_ids'], output_hidden_states=True)\n",
        "\n",
        "      # Use a dictionary comprehension to store the last token's hidden state for each layer\n",
        "      # outputs.hidden_states is a tuple of hidden state tensors (one for each layer).\n",
        "      # The shape of each tensor (hs) is (1, seq_len, dim),\n",
        "      # so the hidden state for the last token is accessed via hs[0, -1] or hs[0][-1].\n",
        "      hidden_states_contents[f\"Prompt_{i+1}\"] = {\n",
        "          layer_idx: hs for layer_idx, hs in enumerate(outputs.hidden_states)\n",
        "    }\n",
        "  return hidden_states_contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECB7hrAf1pXX"
      },
      "outputs": [],
      "source": [
        "def _process_layer(\n",
        "    layer_hidden_state: torch.Tensor,\n",
        "    model_unembed: Any,\n",
        "    tokenizer: Any\n",
        ") -> Tuple[int, float, str]:\n",
        "    # Extract the vector for the last token from the (batch, seq_len, dim) tensor\n",
        "    last_token_hs = layer_hidden_state[0, -1]\n",
        "\n",
        "    # Calculate logits\n",
        "    logits = model_unembed(last_token_hs)\n",
        "\n",
        "    # Calculate the max value and index at once using torch.max\n",
        "    max_logit_val, argmax_index_tensor = torch.max(logits, dim=-1)\n",
        "\n",
        "    argmax_index = argmax_index_tensor.item()\n",
        "    token = tokenizer.convert_ids_to_tokens(argmax_index)\n",
        "\n",
        "    return (argmax_index, max_logit_val.item(), token)\n",
        "\n",
        "def find_argmax(\n",
        "    hidden_states_by_prompt: List[List[torch.Tensor]],\n",
        "    model_unembed: Any,\n",
        "    tokenizer: Any\n",
        ") -> Tuple[List[List[int]], List[List[float]], List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Calculates the argmax token, index, and logit value from the\n",
        "    hidden states for each prompt and layer.\n",
        "    (This function is the same as the previous response and is written correctly.)\n",
        "    \"\"\"\n",
        "    all_indices, all_logits, all_tokens = [], [], []\n",
        "\n",
        "    for prompt_layers_hs in hidden_states_by_prompt:\n",
        "        if not prompt_layers_hs:\n",
        "            all_indices.append([])\n",
        "            all_logits.append([])\n",
        "            all_tokens.append([])\n",
        "            continue\n",
        "\n",
        "        layer_results = [_process_layer(layer_hs, model_unembed, tokenizer) for layer_hs in prompt_layers_hs]\n",
        "        indices, logits, tokens = zip(*layer_results)\n",
        "\n",
        "        all_indices.append(list(indices))\n",
        "        all_logits.append(list(logits))\n",
        "        all_tokens.append(list(tokens))\n",
        "\n",
        "    return all_indices, all_logits, all_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op17_5UXxLpu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_and_save_results(\n",
        "    data,\n",
        "    prompts,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    model_unembed,\n",
        "    device,\n",
        "    output_filepath\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes data to extract key information from hidden states,\n",
        "    adds an 'answers' key, and saves the final results to a JSON file.\n",
        "    Automatically creates the output directory if it does not exist.\n",
        "\n",
        "    Args:\n",
        "        data (list): A list of data items to process.\n",
        "        prompts (list): A list of prompt templates to use.\n",
        "        model: A Hugging Face transformer model.\n",
        "        tokenizer: The tokenizer corresponding to the model.\n",
        "        model_unembed: The unembedding layer of the model.\n",
        "        device (str): The device to perform computations on ('cpu' or 'cuda').\n",
        "        output_filepath (str): The file path to save the final JSON results.\n",
        "    \"\"\"\n",
        "    final_results = {}\n",
        "\n",
        "    print(\"Starting data processing...\")\n",
        "    # 1. Run model inference and information extraction for each item\n",
        "    for item in tqdm(data, desc=\"Processing data\"):\n",
        "        prompt_list = make_tokenized_prompts(prompts, item, tokenizer, device)\n",
        "        hidden_states_data = process_data(model, prompt_list)\n",
        "        hidden_states_for_find_argmax = [\n",
        "            list(prompt_layers.values()) for prompt_layers in hidden_states_data.values()\n",
        "        ]\n",
        "        indices, logits, tokens = find_argmax(\n",
        "            hidden_states_for_find_argmax, model_unembed, tokenizer\n",
        "        )\n",
        "\n",
        "        item_result_dict = {}\n",
        "        prompt_keys = list(hidden_states_data.keys())\n",
        "        for i, prompt_key in enumerate(prompt_keys):\n",
        "            item_result_dict[prompt_key] = {\n",
        "                \"index\": indices[i],\n",
        "                \"logit\": logits[i],\n",
        "                \"token\": tokens[i],\n",
        "            }\n",
        "        final_results[item['id']] = item_result_dict\n",
        "\n",
        "    print(\"All data processing is complete.\")\n",
        "    print(\"Now adding the 'answers' key and saving to file...\")\n",
        "\n",
        "    try:\n",
        "        # 2. Add the 'answers' key to the top level of each ID\n",
        "        for id_key, prompts_data in final_results.items():\n",
        "            answers = []\n",
        "            prompt_keys_sorted = sorted(\n",
        "                [key for key in prompts_data if key.startswith(\"Prompt_\")],\n",
        "                key=lambda x: int(x.split('_')[1])\n",
        "            )\n",
        "            for p_key in prompt_keys_sorted:\n",
        "                token_list = prompts_data.get(p_key, {}).get(\"token\", [])\n",
        "                if token_list:\n",
        "                    answers.append(token_list[-1]) # Get the last token\n",
        "                else:\n",
        "                    answers.append(None)\n",
        "\n",
        "            # Create a new ordered dictionary with 'answers' first\n",
        "            new_ordered_data = {'answers': answers}\n",
        "            for key, value in prompts_data.items():\n",
        "                new_ordered_data[key] = value\n",
        "            final_results[id_key] = new_ordered_data\n",
        "\n",
        "        # 3. Check the directory path for the output file, create if it doesn't exist\n",
        "        output_dir = os.path.dirname(output_filepath)\n",
        "        if output_dir and not os.path.exists(output_dir): # Check if output_dir is not empty and doesn't exist\n",
        "            print(f\"INFO: Output directory '{output_dir}' does not exist. Creating it.\")\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # 4. Save the final results to the new file\n",
        "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"‚úÖ Final results successfully saved to '{output_filepath}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while saving the file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du55HnoFLE5z"
      },
      "source": [
        "#4. ‚úÖ Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y8XzKFiLJ_7",
        "outputId": "55819bef-7ddc-4c56-ae53-45b3aad4dd5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨Î•º ÏãúÏûëÌï©ÎãàÎã§...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ Ï§ë:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 970/977 [20:56<00:09,  1.30s/it]"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "process_and_save_results(\n",
        "        data=data,\n",
        "        prompts=prompts,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        model_unembed=model_unembed,\n",
        "        device=device,\n",
        "        output_filepath=save_path + \"/data_ilt.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhFBKq_63y_l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}